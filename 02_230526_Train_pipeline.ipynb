{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCAD-RT training pipeline            \n",
    "<img src=\"https://github.com/STAR-811/DeepCAD-RT-old/blob/main/images/logo-new.png?raw=true\" width = \"650\" height = \"180\" align=right />\n",
    "\n",
    "This file will demonstrate the basic pipeline for training DeepCAD-RT. A TIFF file will be downloaded automatically to be the example data. More information about the method and relevant results can be found in the companion paperï¼š\n",
    "\n",
    "**Real-time denoising of fluorescence time-lapse imaging enables high-sensitivity observations of biological dynamics beyond the shot-noise limit. bioRxiv (2022).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepcad.train_collection import training_class\n",
    "from deepcad.movie_display import display\n",
    "from deepcad.utils import get_first_filename,download_demo\n",
    "import os\n",
    "import glob\n",
    "from tifffile import imread, TiffFile, imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select file(s) to be processed (download if not exist)\n",
    "The `download_demo` function will download a demo file and return the full path of it. This demo file will be stored in `/datasets`. If you want to use your own data for training, please create a new folder in `/datasets` and copy your data into it. \n",
    "Then, just change `datasets_path` into the name of your dataset folder. All TIFF files inside the dataset folder will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording data: \n",
      "analysis data: 2023-05-26 16:03:05.694461\n",
      "behaviorInfo: \n",
      "sensor: GCaMP7.09\n",
      "recTarget: Soma\n",
      "fs: 7.65\n",
      "resolution: 2048\n",
      "analysisDir: /mnt/ssd1/ysaito/suite2p-pipeline-main/02_analysis/i166-m2_Exp01/01_ROI_detection_DeepCAD_all\n",
      "dataDir_for_DeepCAD: /mnt/ssd1/ysaito/suite2p-pipeline-main/02_analysis/i166-m2_Exp01/01_ROI_detection_DeepCAD_all/suite2p/plane0/reg_tif/data_all\n"
     ]
    }
   ],
   "source": [
    "metainfo = h5py.File(os.getcwd()+'/00_Metafile/metafile.h5', 'r')\n",
    "\n",
    "recordingDate = metainfo['sessionInfo/recordingDate'][()].decode()\n",
    "mouseID = metainfo['sessionInfo/mouseID'][()].decode()\n",
    "behaviorInfo = metainfo['sessionInfo/behaviorInfo'][()].decode()\n",
    "recTarget = metainfo['sessionInfo/recTarget'][()].decode()\n",
    "sensor = metainfo['sessionInfo/sensor'][()].decode()\n",
    "fs = metainfo['sessionInfo/fs'][()]\n",
    "resolution = metainfo['sessionInfo/resolution'][()]\n",
    "analysisDate = metainfo['sessionInfo/analysisDate'][()].decode()\n",
    "\n",
    "\n",
    "analysisDir = metainfo['sessionInfo/analysisDir'][()].decode()+'/01_ROI_detection_DeepCAD_all' \n",
    "datasets_path = analysisDir+'/suite2p/plane0/reg_tif/data_all'\n",
    "metainfo.close()\n",
    "\n",
    "print('recording data:', recordingDate)\n",
    "print('analysis data:', analysisDate)\n",
    "print('behaviorInfo:', behaviorInfo)\n",
    "print('sensor:', sensor)\n",
    "print('recTarget:', recTarget)\n",
    "print('fs:', fs)\n",
    "print('resolution:', resolution)\n",
    "\n",
    "print('analysisDir:', analysisDir)\n",
    "print('dataDir_for_DeepCAD:', datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters for training\n",
    "Default setting shows the parameters for the demo file, which are also appropriate for most data. You can change these parameters according to your data and device. To visualize the training process, you can set the flags `visualize_images_per_epoch` and `save_test_images_per_epoch` according to your demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 20                # number of training epochs\n",
    "GPU = '0'                   # the index of GPU you will use (e.g. '0', '0,1', '0,1,2')\n",
    "train_datasets_size = 3000  # datasets size for training (how many 3D patches)\n",
    "patch_xy = 150              # the width and height of 3D patches\n",
    "patch_t = 150               # the time dimension (frames) of 3D patches\n",
    "overlap_factor = 0.4        # the overlap factor between two adjacent patches\n",
    "pth_dir = 'pth'           # the path for pth file and result images \n",
    "num_workers = 0             # if you use Windows system, set this to 0.\n",
    "\n",
    "# Setup some parameters for result visualization during training period (optional)\n",
    "visualize_images_per_epoch = False  # whether to show result images after each epoch\n",
    "save_test_images_per_epoch = False  # whether to save result images after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Show the input low-SNR data  (optional)\n",
    "Play an input video (optional). This will load the video into memory and it is not an indispensable step. OpenCV library was used for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images = False\n",
    "\n",
    "if display_images:\n",
    "    display_filename = get_first_filename(datasets_path)\n",
    "    print('\\033[1;31mDisplaying the first raw file -----> \\033[0m')\n",
    "    print(display_filename)\n",
    "    display_length = 300     # how many frames to display\n",
    "    # normalize the image and display\n",
    "    display(display_filename, display_length=display_length, norm_min_percent=1, norm_max_percent=98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training object\n",
    "This will creat a training object by passing all parameters as a dictionary. Parameters not specified in the dictionary will use their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mTraining parameters -----> \u001b[0m\n",
      "{'overlap_factor': 0.4, 'datasets_path': '/mnt/ssd1/ysaito/suite2p-pipeline-main/02_analysis/i166-m2_Exp01/01_ROI_detection_DeepCAD_all/suite2p/plane0/reg_tif/data_all', 'n_epochs': 20, 'fmap': 16, 'output_dir': './results', 'pth_dir': 'pth', 'onnx_dir': './onnx', 'batch_size': 1, 'patch_t': 150, 'patch_x': 150, 'patch_y': 150, 'gap_y': 90, 'gap_x': 90, 'gap_t': 90, 'lr': 5e-05, 'b1': 0.5, 'b2': 0.999, 'GPU': '0', 'ngpu': 1, 'num_workers': 0, 'scale_factor': 1, 'train_datasets_size': 3000, 'select_img_num': 1000000, 'test_datasize': 400, 'visualize_images_per_epoch': False, 'save_test_images_per_epoch': False, 'colab_display': False, 'result_display': ''}\n"
     ]
    }
   ],
   "source": [
    "train_dict = {\n",
    "    # dataset dependent parameters\n",
    "    'patch_x': patch_xy,                          # the width of 3D patches\n",
    "    'patch_y': patch_xy,                          # the height of 3D patches\n",
    "    'patch_t': patch_t,                           # the time dimension (frames) of 3D patches\n",
    "    'overlap_factor':overlap_factor,              # overlap factor\n",
    "    'scale_factor': 1,                            # the factor for image intensity scaling\n",
    "    'select_img_num': 1000000,                    # select the number of frames used for training (use all frames by default)\n",
    "    'train_datasets_size': train_datasets_size,   # datasets size for training (how many 3D patches)\n",
    "    'datasets_path': datasets_path,               # folder containing files for training\n",
    "    'pth_dir': pth_dir,                           # the path for pth file and result images \n",
    "    \n",
    "    # network related parameters\n",
    "    'n_epochs': n_epochs,                         # the number of training epochs\n",
    "    'lr': 0.00005,                                # learning rate\n",
    "    'b1': 0.5,                                    # Adam: bata1\n",
    "    'b2': 0.999,                                  # Adam: bata2\n",
    "    'fmap': 16,                                   # model complexity\n",
    "    'GPU': GPU,                                   # GPU index\n",
    "    'num_workers': num_workers,                   # if you use Windows system, set this to 0.\n",
    "    'visualize_images_per_epoch': visualize_images_per_epoch,   # whether to show result images after each epoch\n",
    "    'save_test_images_per_epoch': save_test_images_per_epoch    # whether to save result images after each epoch\n",
    "}\n",
    "\n",
    "tc = training_class(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training process\n",
    "\n",
    "Here we lanuch the training process. The model of each epoch will be saved in the `/pth` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mImage list for training -----> \u001b[0m\n",
      "Total stack number ----->  1\n",
      "Noise image name ----->  registered.tif\n",
      "Noise image shape ----->  (8000, 2048, 2048)\n",
      "\u001b[1;31mUsing 1 GPU(s) for training -----> \u001b[0m\n",
      "[Epoch 1/20] [Batch 3388/3388] [Total loss: 325453.31, L1 Loss: 630.18, L2 Loss: 650276.44] [ETA: 8:00:01] [Time cost: 1525 s]         \n",
      "[Epoch 2/20] [Batch 3388/3388] [Total loss: 486950.69, L1 Loss: 770.43, L2 Loss: 973130.94] [ETA: 7:25:50] [Time cost: 3052 s]         \n",
      "[Epoch 3/20] [Batch 3388/3388] [Total loss: 321193.66, L1 Loss: 618.11, L2 Loss: 641769.19] [ETA: 6:59:15] [Time cost: 4578 s]          \n",
      "[Epoch 4/20] [Batch 3388/3388] [Total loss: 865988.88, L1 Loss: 1026.68, L2 Loss: 1730951.12] [ETA: 6:34:31] [Time cost: 6105 s]       \n",
      "[Epoch 5/20] [Batch 3388/3388] [Total loss: 270377.47, L1 Loss: 568.22, L2 Loss: 540186.75] [ETA: 6:04:11] [Time cost: 7628 s]         \n",
      "[Epoch 6/20] [Batch 3388/3388] [Total loss: 166310.00, L1 Loss: 436.09, L2 Loss: 332183.91] [ETA: 6:06:13] [Time cost: 9151 s]         \n",
      "[Epoch 7/20] [Batch 3388/3388] [Total loss: 200312.47, L1 Loss: 497.19, L2 Loss: 400127.75] [ETA: 5:26:09] [Time cost: 10673 s]         \n",
      "[Epoch 8/20] [Batch 3388/3388] [Total loss: 716504.06, L1 Loss: 918.20, L2 Loss: 1432089.88] [ETA: 4:55:16] [Time cost: 12196 s]        \n",
      "[Epoch 9/20] [Batch 3388/3388] [Total loss: 435421.56, L1 Loss: 729.20, L2 Loss: 870113.94] [ETA: 5:17:17] [Time cost: 13717 s]         \n",
      "[Epoch 10/20] [Batch 3388/3388] [Total loss: 857799.56, L1 Loss: 1025.83, L2 Loss: 1714573.25] [ETA: 4:07:53] [Time cost: 15241 s]       \n",
      "[Epoch 11/20] [Batch 3388/3388] [Total loss: 389027.12, L1 Loss: 672.50, L2 Loss: 777381.75] [ETA: 3:53:25] [Time cost: 16767 s]         \n",
      "[Epoch 12/20] [Batch 3388/3388] [Total loss: 1183979.88, L1 Loss: 1213.75, L2 Loss: 2366746.00] [ETA: 3:17:44] [Time cost: 18291 s]      \n",
      "[Epoch 13/20] [Batch 3388/3388] [Total loss: 54117.56, L1 Loss: 251.43, L2 Loss: 107983.69] [ETA: 2:55:29] [Time cost: 19815 s]          \n",
      "[Epoch 14/20] [Batch 3388/3388] [Total loss: 626866.56, L1 Loss: 854.80, L2 Loss: 1252878.38] [ETA: 2:35:42] [Time cost: 21336 s]        \n",
      "[Epoch 15/20] [Batch 3388/3388] [Total loss: 597798.31, L1 Loss: 853.67, L2 Loss: 1194743.00] [ETA: 2:06:01] [Time cost: 22854 s]        \n",
      "[Epoch 16/20] [Batch 3388/3388] [Total loss: 299071.94, L1 Loss: 574.86, L2 Loss: 597569.00] [ETA: 1:38:59] [Time cost: 24378 s]         \n",
      "[Epoch 17/20] [Batch 3388/3388] [Total loss: 353075.50, L1 Loss: 647.48, L2 Loss: 705503.50] [ETA: 1:12:14] [Time cost: 25898 s]         \n",
      "[Epoch 18/20] [Batch 3388/3388] [Total loss: 90397.61, L1 Loss: 316.43, L2 Loss: 180478.80] [ETA: 0:52:54] [Time cost: 27418 s]          \n",
      "[Epoch 19/20] [Batch 3388/3388] [Total loss: 347858.22, L1 Loss: 625.41, L2 Loss: 695091.00] [ETA: 0:24:36] [Time cost: 28937 s]         \n",
      "[Epoch 20/20] [Batch 3388/3388] [Total loss: 44912.15, L1 Loss: 225.64, L2 Loss: 89598.66] [ETA: 0:00:00] [Time cost: 30462 s]           \n",
      " Train finished. Save all models to disk.\n",
      "CPU times: user 15h 8min 8s, sys: 18min 14s, total: 15h 26min 23s\n",
      "Wall time: 8h 35min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
